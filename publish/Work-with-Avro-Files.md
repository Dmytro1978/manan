# Work with Avro Files

## Sqoop command to store data in Avro format

Apache Sqoop supports Avro file format. To store data in Avro following parameters should be used: 

```python
--as-avrodatafile
--compression-codec snappy #(optional)
```
The template of Sqoop command is following:

```python
    sqoop import \
            --bindir ./ \
            --connect 'jdbc:sqlserver://<host>:<port>;databasename=<database_name>' \
                # 'jdbc:oracle:thin:<username>/password@<host>:<port>/<instance_name>'
                # 'jdbc:jtds:sqlserver://<host>:<port>/<database_name>' - this is for SQL Server 2000
            --username <username> \
            --driver <driver_class> # used to explicitly setting the driver
                                    # example: --driver net.sourceforge.jtds.jdbc.Driver
            --connection-manager # 
              #example: --connection-manager org.apache.sqoop.manager.SQLServerManager
            --password <password> \
            --num-mappers <n> \
            --fields-terminated-by '\t' \ # used for CSV, useless in Avro
            --lines-terminated-by '\n' \  # used for CSV, useless in Avro 
            --as-avrodatafile \           # to store data in Avro format
            --compression-codec snappy \ 
            --options-file ${work_dir}/${sqoop_opt_file} \
            --split-by <field_name> \ # only used if number of mappers > 1
            --target-dir s3://<path> \
               # example for HDFS: --target-dir hdfs:///<path>
            --null-string '' \
            --null-non-string ''
            --boundary-query # if used then --split-by should also be present 
```

Example:

```python
    sqoop import \
     -Dmapreduce.job.user.classpath.first=true \
     --connect "jdbc:oracle:thin:admin_user/Passw0rd1@oradbinstance-1.cxgoy0cgjzah.us-west-2.rds.amazonaws.com:1521/orcl" \
     --num-mappers 1 \
     --query 'select * from employee where $CONDITIONS' \
     --target-dir s3://mdmytro-dw/staging/employee \
     --as-avrodatafile \
     --compression-codec snappy \
     --null-string '' \
     --null-non-string '' 
```

Sqoop can transfer data to either Hadoop (HDFS) or AWS (S3). To query transferred data you need to create tables on top of physical files. If the data was transferred to Hadoop you can create Hive tables. If the data was transferred to S3 you can create either Hive tables of Amazon Athena tables. In both cases you will need a table schema which you can retrieve from physical files.
Starting from version 1.4.7 (EMR 5.14.0, Hadoop distribution: Amazon 2.8.3) Sqoop automatically creates table schema and stores it in “AutoGeneratedSchema.avsc” file in the same folder. 
If Sqoop 1.4.6 (EMR 5.13.0) or lower is used then the table schema can be retrieved manually:

```python
java -jar avro-tools-1.8.1.jar getschema part-m-00000.avro > employee.avsc
```

Note, that Avro file (part-m-00000.avro) should only be local or in HDFS, not in S3.

## Create Avro table in Hive

To create an Avro table in Hive (on Hadoop Cluster or on EMR) you have to provide a table schema location retrieved from the Avro data file: 

```sql
CREATE EXTERNAL TABLE employee
STORED AS AVRO
LOCATION '/user/hive/warehouse/employee'
TBLPROPERTIES('avro.schema.url'='hdfs:///user/hive/warehouse/avsc/employee.avsc');
```
You can also create a table in S3:
```sql
CREATE EXTERNAL TABLE employee
STORED AS AVRO
location 's3://mdmytro-dw/staging/employee'
TBLPROPERTIES ('avro.schema.url'='hdfs:///user/hive/warehouse/avsc/employee.avsc');
```
You can even keep a table schema in S3:
```sql
CREATE EXTERNAL TABLE employee
STORED AS AVRO
location 's3://mdmytro-dw/staging/employee'
TBLPROPERTIES ('avro.schema.url'='s3://mdmytro-dw/staging/avsc/employee.avsc');
```

The Avro schema for EMPLOYEE table looks like this:
```json
    {
      "type" : "record",
      "name" : "AutoGeneratedSchema",
      "doc" : "Sqoop import of QueryResult",
      "fields" : [ {
        "name" : "ID",
        "type" : [ "null", "string" ],
        "default" : null,
        "columnName" : "ID",
        "sqlType" : "2"
      }, {
        "name" : "NAME",
        "type" : [ "null", "string" ],
        "default" : null,
        "columnName" : "NAME",
        "sqlType" : "12"
      }, {
        "name" : "AGE",
        "type" : [ "null", "string" ],
        "default" : null,
        "columnName" : "AGE",
        "sqlType" : "2"
      }, {
        "name" : "GEN",
        "type" : [ "null", "string" ],
        "default" : null,
        "columnName" : "GEN",
        "sqlType" : "12"
      }, {
        "name" : "CREATE_DATE",
        "type" : [ "null", "long" ],
        "default" : null,
        "columnName" : "CREATE_DATE",
        "sqlType" : "93"
      }, {
        "name" : "PROCESS_NAME",
        "type" : [ "null", "string" ],
        "default" : null,
        "columnName" : "PROCESS_NAME",
        "sqlType" : "12"
      }, {
        "name" : "UPDATE_DATE",
        "type" : [ "null", "long" ],
        "default" : null,
        "columnName" : "UPDATE_DATE",
        "sqlType" : "93"
      } ],
      "tableName" : "QueryResult"
    }
```

Note, that all timestamp columns are defined as ***long***.

## Create Avro table in Amazon Athena

Amazon Athena does not support table property ***avro.schema.url*** - the schema needs to be added explicitly in ***avro.schema.literal***:
```sql
    CREATE EXTERNAL TABLE employee
    (
      ID string,
      NAME string,
      AGE string,
      GEN string,
      CREATE_DATE bigint,
      PROCESS_NAME string,
      UPDATE_DATE bigint
    )
    STORED AS AVRO
    LOCATION 's3://mdmytro-dw/staging/employees'
    TBLPROPERTIES (
    'avro.schema.literal'='
    {
        "type" : "record",
        "name" : "AutoGeneratedSchema",
        "doc" : "Sqoop import of QueryResult",
        "fields" : [ {
          "name" : "ID",
          "type" : [ "null", "string" ],
          "default" : null,
          "columnName" : "ID",
          "sqlType" : "2"
        }, {
          "name" : "NAME",
          "type" : [ "null", "string" ],
          "default" : null,
          "columnName" : "NAME",
          "sqlType" : "12"
        }, {
          "name" : "AGE",
          "type" : [ "null", "string" ],
          "default" : null,
          "columnName" : "AGE",
          "sqlType" : "2"
        }, {
          "name" : "GEN",
          "type" : [ "null", "string" ],
          "default" : null,
          "columnName" : "GEN",
          "sqlType" : "12"
        }, {
          "name" : "CREATE_DATE",
          "type" : [ "null", "long" ],
          "default" : null,
          "columnName" : "CREATE_DATE",
          "sqlType" : "93"
        }, {
          "name" : "PROCESS_NAME",
          "type" : [ "null", "string" ],
          "default" : null,
          "columnName" : "PROCESS_NAME",
          "sqlType" : "12"
        }, {
          "name" : "UPDATE_DATE",
          "type" : [ "null", "long" ],
          "default" : null,
          "columnName" : "UPDATE_DATE",
          "sqlType" : "93"
        } ],
        "tableName" : "QueryResult"
      }
    ');
```
Note, that all timestamp columns in the table definition are defined as ***bigint*** . The explanation of this you can find below. 


## Working with timestamps in Avro

When Sqoop imports data from Oracle to Avro (using ***--as-avrodatafile***) it stores all "timestamp" values in Unix time format (***long***). 

### Hive
No changes when creating Avro table in Hive:

```sql
CREATE EXTERNAL TABLE employee
STORED AS AVRO
LOCATION '/user/hive/warehouse/employee'
TBLPROPERTIES ('avro.schema.url'='hdfs:///user/hive/warehouse/avsc/employee.avsc');
```

When querying the data, just convert milliseconds to date/time:
```sql
select from_unixtime(create_date div 1000) as create_date
from employee;
```
The result dataset without using timestamp conversion: 

```sql
hive> select id, name, age, gen, create_date, process_name, update_date 
    > from employee limit 2;
OK
id  name    age  gen  create_date    process_name  update_date
--  ----    ---  ---  -----------    ------------  -----------
2   John    30   M    1538265652000  BACKFILL      1538269659000
3   Jennie  25   F    1538265652000  BACKFILL      1538269659000
```

The result dataset using timestamp conversion:
```sql
    hive> select 
        >     id, name, age, gen, 
        >     from_unixtime(create_date div 1000) as create_date, 
        >     process_name, 
        >     from_unixtime(update_date div 1000) as update_date 
        > from employee limit 2;
    OK
    id  name    age  gen  create_date          process_name  update_date
    --  ----    ---  ---  -----------          ------------  -----------
    2   John    30   M    2018-09-30 00:00:52  BACKFILL      2018-09-30 01:07:39
    3   Jennie  25   F    2018-09-30 00:00:52  BACKFILL      2018-09-30 01:07:39
```
**Important**: In Hive if reserved words are used as column names (like timestamp) use backquotes:

```sql
select from_unixtime(`timestamp` div 1000) as time_stamp 
from employee;
```
### Athena
When creating Athena table all ***long*** fields should be created as ***bigint*** in ***create table*** statement (not in Avro schema!):
```sql
    CREATE EXTERNAL TABLE employee
    (
      ID string,
      NAME string,
      AGE string,
      GEN string,
      CREATE_DATE bigint,
      PROCESS_NAME string,
      UPDATE_DATE bigint
    )
    STORED AS AVRO
    LOCATION 's3://mdmytro-dw/staging/employee'
    TBLPROPERTIES (
    'avro.schema.literal'='
    {
        "type" : "record",
        "name" : "AutoGeneratedSchema",
        "doc" : "Sqoop import of QueryResult",
        "fields" : [ {
          "name" : "ID",
          "type" : [ "null", "string" ],
          "default" : null,
          "columnName" : "ID",
          "sqlType" : "2"
        }, {
          "name" : "NAME",
          "type" : [ "null", "string" ],
          "default" : null,
          "columnName" : "NAME",
          "sqlType" : "12"
        }, {
          "name" : "AGE",
          "type" : [ "null", "string" ],
          "default" : null,
          "columnName" : "AGE",
          "sqlType" : "2"
        }, {
          "name" : "GEN",
          "type" : [ "null", "string" ],
          "default" : null,
          "columnName" : "GEN",
          "sqlType" : "12"
        }, {
          "name" : "CREATE_DATE",
          "type" : [ "null", "long" ],
          "default" : null,
          "columnName" : "CREATE_DATE",
          "sqlType" : "93"
        }, {
          "name" : "PROCESS_NAME",
          "type" : [ "null", "string" ],
          "default" : null,
          "columnName" : "PROCESS_NAME",
          "sqlType" : "12"
        }, {
          "name" : "UPDATE_DATE",
          "type" : [ "null", "long" ],
          "default" : null,
          "columnName" : "UPDATE_DATE",
          "sqlType" : "93"
        } ],
        "tableName" : "QueryResult"
      }
    ');   
```
When querying the data, convert milliseconds to date/time:
```sql
select from_unixtime(create_date / 1000) as create_date
from employee;
```
The result dataset without using timestamp conversion: 
```sql
select id, name, age, gen, create_date, process_name, update_date 
from employee limit 2;
```
```sql
id  name    age  gen  create_date    process_name  update_date
--  ----    ---  ---  -----------    ------------  -----------
2   John    30	 M    1538265652000  BACKFILL      1538269659000
3   Jennie  25	 F    1538265652000  BACKFILL      1538269659000
```
The result dataset using timestamp conversion:
```sql
select id, name, age, gen,
  from_unixtime(create_date / 1000) as create_date,
  process_name, 
  from_unixtime(update_date / 1000) as update_date
from employee limit 2;
```

```sql
id  name    age  gen  create_date              process_name  update_date
--  ----    ---  ---  -----------              ------------  -----------
2   John    30   M    2018-09-30 00:00:52.000  BACKFILL      2018-09-30 01:07:39.000
3   Jennie  25   F    2018-09-30 00:00:52.000  BACKFILL      2018-09-30 01:07:39.000
```

## Storing Timestamp as Text 

If you do not want to convert timestamp from Unix time every time you run a query, you can store timestamp values as text by adding following parameter to Sqoop

```
    --map-column-java CREATE_DATE=String,UPDATE_DATE=String
```
After applying this parameter and running Sqoop the table schema will look like this:
```json
    {
      "type" : "record",
      "name" : "AutoGeneratedSchema",
      "doc" : "Sqoop import of QueryResult",
      "fields" : [ {
        "name" : "ID",
        "type" : [ "null", "string" ],
        "default" : null,
        "columnName" : "ID",
        "sqlType" : "2"
      }, {
        "name" : "NAME",
        "type" : [ "null", "string" ],
        "default" : null,
        "columnName" : "NAME",
        "sqlType" : "12"
      }, {
        "name" : "AGE",
        "type" : [ "null", "string" ],
        "default" : null,
        "columnName" : "AGE",
        "sqlType" : "2"
      }, {
        "name" : "GEN",
        "type" : [ "null", "string" ],
        "default" : null,
        "columnName" : "GEN",
        "sqlType" : "12"
      }, {
        "name" : "CREATE_DATE",
        "type" : [ "null", "string" ],
        "default" : null,
        "columnName" : "CREATE_DATE",
        "sqlType" : "93"
      }, {
        "name" : "PROCESS_NAME",
        "type" : [ "null", "string" ],
        "default" : null,
        "columnName" : "PROCESS_NAME",
        "sqlType" : "12"
      }, {
        "name" : "UPDATE_DATE",
        "type" : [ "null", "string" ],
        "default" : null,
        "columnName" : "UPDATE_DATE",
        "sqlType" : "93"
      } ],
      "tableName" : "QueryResult"
    }
```
Note, that fields columns in the table schema are defined as string.

Sqoop command for storing timestamp fields in string format:
```python
    sqoop import \
     -Dmapreduce.job.user.classpath.first=true \
     --connect "jdbc:oracle:thin:admin_user/Passw0rd1@oradbinstance-1.cxgoy0cgjzah.us-west-2.rds.amazonaws.com:1521/orcl" \
     --num-mappers 1 \
     --query 'select * from employee where $CONDITIONS' \
     --target-dir hdfs:///mdmytro-dw/staging/employee_ts_str \
     --as-avrodatafile \
     --compression-codec snappy \
     --null-string '' \
     --null-non-string '' \
     --map-column-java CREATE_DATE=String,UPDATE_DATE=String
```
### Hive
Create new table in Hive using this new table schema:
```sql
CREATE EXTERNAL TABLE employee_ts_str
STORED AS AVRO
LOCATION '/user/hive/warehouse/employee_ts_str'
TBLPROPERTIES('avro.schema.url'='hdfs:///user/hive/warehouse/avsc/employee_ts_str.avsc');
```
Select the data without using timestamp conversion:
```sql
hive> select id, name, age, gen, create_date, process_name, update_date
    > from employee_ts_str limit 2;
OK
id  name   age  gen  create_date          process_name  update_date
--  ----   ---  ---  -----------          ------------  -----------
2  John    30   M    2018-09-30 00:00:52  BACKFILL      2018-09-30 01:07:39
3  Jennie  25   F    2018-09-30 00:00:52  BACKFILL      2018-09-30 01:07:39
```
### Athena
Create new table in Amazon Athena using this new table schema:
```sql
    CREATE EXTERNAL TABLE employee_ts_str
    (
      ID string,
      NAME string,
      AGE string,
      GEN string,
      CREATE_DATE string,
      PROCESS_NAME string,
      UPDATE_DATE string
    )
    STORED AS AVRO
    LOCATION 's3://mdmytro-dw/staging/employee_ts_str'
    TBLPROPERTIES (
    'avro.schema.literal'='
    {
        "type" : "record",
        "name" : "AutoGeneratedSchema",
        "doc" : "Sqoop import of QueryResult",
        "fields" : [ {
          "name" : "ID",
          "type" : [ "null", "string" ],
          "default" : null,
          "columnName" : "ID",
          "sqlType" : "2"
        }, {
          "name" : "NAME",
          "type" : [ "null", "string" ],
          "default" : null,
          "columnName" : "NAME",
          "sqlType" : "12"
        }, {
          "name" : "AGE",
          "type" : [ "null", "string" ],
          "default" : null,
          "columnName" : "AGE",
          "sqlType" : "2"
        }, {
          "name" : "GEN",
          "type" : [ "null", "string" ],
          "default" : null,
          "columnName" : "GEN",
          "sqlType" : "12"
        }, {
          "name" : "CREATE_DATE",
          "type" : [ "null", "string" ],
          "default" : null,
          "columnName" : "CREATE_DATE",
          "sqlType" : "93"
        }, {
          "name" : "PROCESS_NAME",
          "type" : [ "null", "string" ],
          "default" : null,
          "columnName" : "PROCESS_NAME",
          "sqlType" : "12"
        }, {
          "name" : "UPDATE_DATE",
          "type" : [ "null", "string" ],
          "default" : null,
          "columnName" : "UPDATE_DATE",
          "sqlType" : "93"
        } ],
        "tableName" : "QueryResult"
      }
    ');  
```
Note, that timestamp columns in the table definition are defined as “string”.

Select the data without using timestamp conversion:
```sql
select id, name, age, gen, create_date, process_name, update_date
from employee_ts_str limit 2;
```
```sql
id  name    age gen  create_date          process_name  update_date
--  ----   ---  ---  -----------          ------------  -----------
2   John    30  M    2018-09-30 00:00:52  BACKFILL      2018-09-30 01:07:39
3   Jennie  25  F    2018-09-30 00:00:52  BACKFILL      2018-09-30 01:07:39
```

## Avro files concatenation

If there are several output files (there were more than one number of mapper ) and you want to combine them into one file you can to use concatenation: 
```python
hadoop jar avro-tools-1.8.1.jar part-m-00000.avro part-m-00001.avro cons_file.avro
```
Files can be local or in S3:
```python
hadoop jar avro-tools-1.8.1.jar concat s3://mdmytro-dw/staging/employee/part-m-00000.avro s3://mdmytro-dw/staging/employee/part-m-00001.avro s3://mdmytro-dw/staging/employee/employee_final.avro
```

## Data types in Java and Hive

Integral types

| **Hive** | **Java** |
| -------- | -------- |
| TINYINT  | byte     |
| SMALLINT | short    |
| INT      | int      |
| BIGINT   | long     |


Floating types are – ***FLOAT***, ***DOUBLE*** & ***DECIMAL***. – Equivalent to Java’s float and double , and SQL’s Decimal respectively.

***DECIMAL(5,2)*** represents total of 5 digits, out of which 2 are decimal digits. Below is the chart for all numeric types with their ranges and examples.
[http://hadooptutorial.info/hive-data-types-examples/](http://hadooptutorial.info/hive-data-types-examples/)

